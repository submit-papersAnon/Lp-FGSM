{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# $l^p$-FGSM in TensorFlow: Mitigating Catastrophic Overfitting in Fast Adversarial Training\n",
        "\n",
        " TensorFlow implementation of the $l^p$-Fast Gradient Sign Method ($l^p$-FGSM), a novel approach presented in the paper \"An $l^p$ Norm Solution to Catastrophic Overfitting in Fast Adversarial Training.\" This notebook aims to provide a comprehensive and interactive exploration of the techniques and insights introduced in our research.\n",
        "\n",
        "## Overview\n",
        "\n",
        "Adversarial training has emerged as a powerful tool for enhancing the robustness of deep neural networks against adversarial attacks. Traditional methods, while effective, often come with considerable computational costs. Fast adversarial training methods like the Fast Gradient Sign Method (FGSM) offer a more efficient alternative. However, they also introduce the challenge of catastrophic overfitting, where models become robust against single-step attacks but remain surprisingly vulnerable to multi-step variants.\n",
        "\n",
        "Our work focuses on this pivotal issue, examining the prevalence of catastrophic overfitting under different norm constraints. Through empirical analysis, we discovered that catastrophic overfitting is more pronounced under the $ l^\\infty $ norm than the $ l^2 $ norm. Building on this insight, we developed the $l^p$-FGSM framework, which generalizes adversarial perturbation creation across various norms. This framework allows a seamless transition from $ l^2 $ to $ l^\\infty $ attacks,\n",
        "\n",
        "In this notebook, we delve into the $l^p$-FGSM method, addressing the challenge of catastrophic overfitting in fast adversarial training and providing a hands-on experience with our proposed solution.\n",
        "\n",
        "### Key Highlights:\n",
        "- Implementation of the $l^p$-FGSM method in TensorFlow.\n",
        "- Detailed exploration of the impact of different $ l^p $ norms on adversarial robustness.\n"
      ],
      "metadata": {
        "id": "idS1qAJ1O5qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers, utils\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.datasets import cifar10, cifar100\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam, AdamW, SGD\n",
        "from tensorflow.keras.callbacks import Callback\n"
      ],
      "metadata": {
        "id": "1EPypNjFFAFw",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global configuration parameters\n",
        "dataset_name = \"CIFAR10\"  # Change this to \"SVHN\" ,\"CIFAR10\" \"CIFAR100\" as needed\n",
        "batch_size = 128\n",
        "epochs=30\n",
        "weight_decay =5e-4  #\n",
        "init_shape = (32, 32, 3)\n",
        "activation = \"relu\"\n",
        "dropout=0.1"
      ],
      "metadata": {
        "id": "QVFiromK8wZY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset Loading\n",
        "\n",
        "seed = 88888888\n",
        "np.random.seed(seed)  # For NumPy random numbers\n",
        "tf.random.set_seed(seed)  # For TensorFlow random numbers\n",
        "\n",
        "# Function to load and preprocess SVHN dataset\n",
        "def load_svhn():\n",
        "    (svhn_train, svhn_test), ds_info = tfds.load(\n",
        "        'svhn_cropped', split=['train', 'test'], as_supervised=True, with_info=True, batch_size=-1)\n",
        "\n",
        "    svhn_train_images, svhn_train_labels = tfds.as_numpy(svhn_train)\n",
        "    svhn_test_images, svhn_test_labels = tfds.as_numpy(svhn_test)\n",
        "\n",
        "    # Normalize and preprocess images\n",
        "    x_train = svhn_train_images.astype(\"float32\") / 255.0\n",
        "    x_test = svhn_test_images.astype(\"float32\") / 255.0\n",
        "\n",
        "    x_train = x_train / np.max(x_train, axis=(1, 2, 3), keepdims=True)\n",
        "    x_test = x_test / np.max(x_test, axis=(1, 2, 3), keepdims=True)\n",
        "\n",
        "    # Replace any division-by-zero\n",
        "    x_train[np.isnan(x_train)] = 0\n",
        "    x_test[np.isnan(x_test)] = 0\n",
        "\n",
        "    # One-hot encode labels\n",
        "    num_classes = ds_info.features['label'].num_classes\n",
        "    y_train = utils.to_categorical(svhn_train_labels, num_classes)\n",
        "    y_test = utils.to_categorical(svhn_test_labels, num_classes)\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test),num_classes\n",
        "\n",
        "# Function to load and preprocess CIFAR-10 dataset\n",
        "def load_cifar10():\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    # Normalize the data\n",
        "    x_train = x_train.astype(\"float32\") / 255.0\n",
        "    x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "    # One-hot encode labels\n",
        "    num_classes = 10\n",
        "    y_train = utils.to_categorical(y_train, num_classes)\n",
        "    y_test = utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test),num_classes\n",
        "\n",
        "# Function to load and preprocess CIFAR-100 dataset\n",
        "def load_cifar100():\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "\n",
        "    # Normalize the data\n",
        "    x_train = x_train.astype(\"float32\") / 255\n",
        "    x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "    # One-hot encode labels\n",
        "    num_classes = 100\n",
        "    y_train = utils.to_categorical(y_train, num_classes)\n",
        "    y_test = utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test),num_classes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to apply custom preprocessing and data augmentation based on the dataset\n",
        "def preprocessing_(dataset_name, x_train, y_train):\n",
        "    if dataset_name == \"SVHN\":\n",
        "        # Custom preprocessing and data augmentation for SVHN\n",
        "        datagen = ImageDataGenerator(\n",
        "            rotation_range=8.0,\n",
        "            zoom_range=[0.95, 1.05],\n",
        "            height_shift_range=0.10,\n",
        "            shear_range=0.15\n",
        "        )\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "        # Custom preprocessing and data augmentation for CIFAR-10\n",
        "        datagen = ImageDataGenerator(\n",
        "            rotation_range=10,\n",
        "            width_shift_range=5./32,\n",
        "            height_shift_range=5./32,\n",
        "            horizontal_flip=True\n",
        "        )\n",
        "    elif dataset_name == \"CIFAR100\":\n",
        "        # Custom preprocessing and data augmentation for CIFAR-100\n",
        "        datagen = ImageDataGenerator(\n",
        "            rotation_range=10,\n",
        "            width_shift_range=5./32,\n",
        "            height_shift_range=5./32,\n",
        "            horizontal_flip=True\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Unknown dataset\")\n",
        "\n",
        "    # Fit the ImageDataGenerator to the training data\n",
        "    datagen.fit(x_train,seed=seed, augment=True)\n",
        "\n",
        "    return datagen\n",
        "\n",
        "# Function to load and preprocess dataset based on name\n",
        "def load_and_preprocess_dataset(dataset_name):\n",
        "    if dataset_name == \"SVHN\":\n",
        "        (x_train, y_train), (x_test, y_test), num_classes = load_svhn()\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "        (x_train, y_train), (x_test, y_test),num_classes = load_cifar10()\n",
        "    elif dataset_name == \"CIFAR100\":\n",
        "        (x_train, y_train), (x_test, y_test),num_classes = load_cifar100()\n",
        "    else:\n",
        "        raise ValueError(\"Unknown dataset\")\n",
        "\n",
        "    # Preprocessing and data augmentation\n",
        "    datagen = preprocessing_(dataset_name, x_train, y_train)\n",
        "\n",
        "    return datagen, (x_train, y_train), (x_test, y_test),num_classes\n",
        "\n",
        "\n",
        "datagen, (x_train, y_train), (x_test, y_test), num_classes = load_and_preprocess_dataset(dataset_name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Iku8pY1RfTJk",
        "outputId": "a54ee5e9-9281-4293-e505-1570a40fb698"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 13s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gopOSjaM5Ktk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title PreActResNet18\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, GlobalAveragePooling2D, Dense, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "def preact_res_block(x, filters, activation='relu', kernel_size=(3, 3), stride=1, weight_decay=0.0, dropout_rate=0.0):\n",
        "    \"\"\"\n",
        "    Creates a pre-activation residual block for PreActResNet.\n",
        "\n",
        "    Args:\n",
        "        x: Input tensor or layer.\n",
        "        filters: Number of filters for the convolution layers.\n",
        "        activation: Activation function to use.\n",
        "        kernel_size: Size of the convolution kernel.\n",
        "        stride: Stride size for the convolution.\n",
        "        weight_decay: L2 regularization factor.\n",
        "        dropout_rate: Dropout rate.\n",
        "\n",
        "    Returns:\n",
        "        A tensor representing the output of the residual block.\n",
        "    \"\"\"\n",
        "    shortcut = x\n",
        "\n",
        "    # Applying batch normalization, activation, and convolution twice\n",
        "    for _ in range(2):\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(activation)(x)\n",
        "        x = Conv2D(filters, kernel_size, strides=stride if _ == 0 else 1, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x)\n",
        "        if dropout_rate > 0.0:\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Adjusting shortcut path for dimensionality matching (if needed)\n",
        "    if stride != 1 or shortcut.shape[-1] != filters:\n",
        "        shortcut = Conv2D(filters, (1, 1), strides=stride, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(shortcut)\n",
        "\n",
        "    x = Add()([shortcut, x])  # Skip connection (element-wise addition)\n",
        "    return x\n",
        "\n",
        "def PreActResNet18(input_shape, num_classes=10, activation='relu', weight_decay=0.0, dropout_rate=0.0):\n",
        "    \"\"\"\n",
        "    Constructs a PreActResNet18 model.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Shape of the input data.\n",
        "        num_classes: Number of classes for the output layer.\n",
        "        activation: Activation function to use in the blocks.\n",
        "        weight_decay: L2 regularization factor.\n",
        "        dropout_rate: Dropout rate.\n",
        "\n",
        "    Returns:\n",
        "        A PreActResNet18 model.\n",
        "    \"\"\"\n",
        "    input = Input(input_shape)\n",
        "    x = Conv2D(64, (7, 7), strides=2, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(2.0 * (input - 0.5))\n",
        "    x = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n",
        "\n",
        "    # Constructing ResNet blocks with specified activation function\n",
        "    for filters, repetitions, use_strided_conv in zip([64, 128, 256, 512], [2, 2, 2, 2], [False, True, True, True]):\n",
        "        for i in range(repetitions):\n",
        "            x = preact_res_block(x, filters, activation, stride=2 if i == 0 and use_strided_conv else 1, weight_decay=weight_decay, dropout_rate=dropout_rate)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    if dropout_rate > 0.0:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "    x = Dense(num_classes, kernel_regularizer=l2(weight_decay))(x)\n",
        "    x = Activation('softmax')(x)  # Softmax activation for classification\n",
        "\n",
        "    return Model(input, x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title WideResNet\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Add, Activation, Dropout, Flatten, Dense, Conv2D, AveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "def initial_conv(input, weight_decay, activation='relu'):\n",
        "    \"\"\"\n",
        "    Initial convolutional layer for WideResNet.\n",
        "\n",
        "    Args:\n",
        "        input: Input tensor or layer.\n",
        "        weight_decay: L2 regularization factor.\n",
        "        activation: Activation function to use.\n",
        "\n",
        "    Returns:\n",
        "        Tensor after applying Conv2D, BatchNormalization, and the specified Activation.\n",
        "    \"\"\"\n",
        "    x = Conv2D(16, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay), use_bias=False)(input)\n",
        "    x = BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation(activation)(x)\n",
        "    return x\n",
        "\n",
        "def expand_conv(init, base, k, strides=(1, 1), weight_decay=0.0, activation='relu'):\n",
        "    \"\"\"\n",
        "    Expanding convolution layer in WideResNet. Increases the dimensionality of the input tensor.\n",
        "\n",
        "    Args:\n",
        "        init: Initial tensor or input layer.\n",
        "        base: Number of base filters.\n",
        "        k: Width factor for scaling the number of filters.\n",
        "        strides: Convolution strides.\n",
        "        weight_decay: L2 regularization factor.\n",
        "        activation: Activation function to use.\n",
        "\n",
        "    Returns:\n",
        "        Tensor after applying Conv2D, BatchNormalization, and Activation.\n",
        "    \"\"\"\n",
        "    x = Conv2D(base * k, (3, 3), padding='same', strides=strides, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay), use_bias=False)(init)\n",
        "    x = BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation(activation)(x)\n",
        "\n",
        "    x = Conv2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay), use_bias=False)(x)\n",
        "\n",
        "    # Creating a shortcut path to implement the skip connection\n",
        "    skip = Conv2D(base * k, (1, 1), padding='same', strides=strides, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay), use_bias=False)(init)\n",
        "    m = Add()([x, skip])  # Adding the skip connection\n",
        "    return m\n",
        "\n",
        "def conv_block(input, k, base, dropout, weight_decay, activation='relu'):\n",
        "    \"\"\"\n",
        "    Standard convolutional block for WideResNet.\n",
        "\n",
        "    Args:\n",
        "        input: Input tensor or layer.\n",
        "        k: Width factor for scaling the number of filters.\n",
        "        base: Number of base filters.\n",
        "        dropout: Dropout rate.\n",
        "        weight_decay: L2 regularization factor.\n",
        "        activation: Activation function to use.\n",
        "\n",
        "    Returns:\n",
        "        Tensor after applying Conv2D, BatchNormalization, Activation, and optionally Dropout.\n",
        "    \"\"\"\n",
        "    init = input\n",
        "    x = BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation(activation)(x)\n",
        "    x = Conv2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay), use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0:\n",
        "        x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation(activation)(x)\n",
        "    x = Conv2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay), use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def create_wide_residual_network(input_dim, nb_classes=100, N=2, k=1, dropout=0.0, verbose=1, weight_decay=1e-4, activation='relu'):\n",
        "    \"\"\"\n",
        "    Creates a Wide Residual Network (WideResNet) with specified parameters.\n",
        "\n",
        "    Args:\n",
        "        input_dim: Dimension of the input data.\n",
        "        nb_classes: Number of output classes for the network.\n",
        "        N: Number of blocks in each group.\n",
        "        k: Width factor for scaling the number of filters.\n",
        "        dropout: Dropout rate.\n",
        "        verbose: Verbosity mode.\n",
        "        weight_decay: L2 regularization factor.\n",
        "        activation: Activation function to use.\n",
        "\n",
        "    Returns:\n",
        "        A WideResNet model.\n",
        "    \"\"\"\n",
        "    ip = Input(shape=input_dim)\n",
        "    x = initial_conv(ip, weight_decay, activation=activation)\n",
        "    nb_conv = 4  # Initial convolutional layer\n",
        "\n",
        "    # First group of blocks\n",
        "    x = expand_conv(x, 16, k, weight_decay=weight_decay, activation=activation)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv_block(x, k, 16, dropout, weight_decay, activation=activation)\n",
        "        nb_conv += 2\n",
        "\n",
        "    # Second group of blocks\n",
        "    x = expand_conv(x, 32, k, strides=(2, 2), weight_decay=weight_decay, activation=activation)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv_block(x, k, 32, dropout, weight_decay, activation=activation)\n",
        "        nb_conv += 2\n",
        "\n",
        "    # Third group of blocks\n",
        "    x = expand_conv(x, 64, k, strides=(2, 2), weight_decay=weight_decay, activation=activation)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv_block(x, k, 64, dropout, weight_decay, activation=activation)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation(activation)(x)\n",
        "\n",
        "    x = AveragePooling2D((8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(nb_classes, kernel_regularizer=l2(weight_decay))(x)\n",
        "    x = Activation('softmax')(x)\n",
        "    model = Model(ip, x)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Wide Residual Network-%d-%d created.\" % (nb_conv, k))\n",
        "    return model\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JHsN0Ykp741O"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Pretraining\n",
        "# Assuming dataset_name is a string variable that contains the name of the dataset\n",
        "if dataset_name == 'SVHN':\n",
        "    # Instantiate PreActResNet18 for SVHN\n",
        "    model = PreActResNet18(input_shape=init_shape, num_classes=10, weight_decay=weight_decay, dropout_rate=dropout, activation=activation)\n",
        "\n",
        "elif dataset_name == 'CIFAR10':\n",
        "    # Create WRN-28-10 for CIFAR10\n",
        "    model = create_wide_residual_network(init_shape, nb_classes=10, N=4, k=8, dropout=dropout, weight_decay=weight_decay, activation=activation)\n",
        "\n",
        "elif dataset_name == 'CIFAR100':\n",
        "    # Create WRN-28-10 for CIFAR100\n",
        "    model = create_wide_residual_network(init_shape, nb_classes=100, N=4, k=8, dropout=dropout, weight_decay=weight_decay, activation=activation)\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"Unknown dataset\")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "pretrain=False # Change this as needed\n",
        "load_saved_model = False  # Change this as needed\n",
        "\n",
        "if pretrain: # In case you want to force local convexity\n",
        "    if load_saved_model:\n",
        "        # Load saved model and evaluate\n",
        "        model = load_model('init_model_pretrained.h5')\n",
        "        model.evaluate(x_test, y_test)\n",
        "    else:\n",
        "        # Train the model\n",
        "        history = model.fit(\n",
        "            datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "            validation_data=(x_test, y_test),\n",
        "            epochs=epochs\n",
        "            )\n",
        "        model.save('init_model_pretrained.h5')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-2NOCUkojO3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "323bb07b-d78b-46d8-d39f-8d6ad188b4e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wide Residual Network-28-8 created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variants of cross entropy loss\n",
        "ccs = tf.keras.losses.CategoricalCrossentropy(reduction='sum') # reduction sum\n",
        "cce = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE) # reduction none to get access to each loss"
      ],
      "metadata": {
        "id": "QVOKy3TUUJR6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PGD attack\n",
        "\n",
        "def clip_epsilon(tensor, epsilon):\n",
        "    \"\"\"\n",
        "    Clips the input tensor values to be within the range [-epsilon, epsilon].\n",
        "\n",
        "    :param tensor: Input tensor.\n",
        "    :param epsilon: Maximum allowed absolute value for elements in the tensor.\n",
        "    :return: Tensor with values clipped to the specified range.\n",
        "    \"\"\"\n",
        "    return tf.clip_by_value(tensor, -epsilon, epsilon)\n",
        "\n",
        "def pgd_attack(model, x, y, epsilon, alpha, attack_iters, clip_min=0.0, clip_max=1.0):\n",
        "    \"\"\"\n",
        "    Performs the Projected Gradient Descent (PGD) attack on a batch of images.\n",
        "\n",
        "    Args:\n",
        "        model: The neural network model to attack.\n",
        "        x: Input images (batch).\n",
        "        y: True labels for x.\n",
        "        epsilon: The maximum perturbation allowed (L-infinity norm).\n",
        "        alpha: Step size for each iteration of the attack.\n",
        "        attack_iters: Number of iterations for the attack.\n",
        "        clip_min: Minimum pixel value after perturbation.\n",
        "        clip_max: Maximum pixel value after perturbation.\n",
        "\n",
        "    Returns:\n",
        "        A batch of adversarial images generated from the input images.\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize epsilon and alpha according to the image scale (0-255)\n",
        "    epsilon = epsilon / 255.0\n",
        "    alpha = alpha / 255.0\n",
        "\n",
        "    # Initialize adversarial images with random perturbations\n",
        "    adv_x = x + tf.random.uniform(tf.shape(x), minval=-epsilon, maxval=epsilon)\n",
        "    adv_x = tf.clip_by_value(adv_x, clip_min, clip_max)  # Ensure they stay within valid pixel range\n",
        "\n",
        "    # Iteratively apply the PGD attack\n",
        "    for _ in tf.range(attack_iters):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(adv_x)  # Watch the adversarial examples for gradient computation\n",
        "            logits = model(adv_x)  # Compute the model's output on the adversarial examples\n",
        "            loss = model.compiled_loss(y, logits)  # Calculate loss\n",
        "\n",
        "        # Compute gradients of the loss w.r.t. adversarial examples\n",
        "        gradients = tape.gradient(loss, adv_x)\n",
        "\n",
        "        # Update adversarial examples using the sign of the gradients\n",
        "        adv_x = adv_x + alpha * tf.sign(gradients)\n",
        "\n",
        "        # Clip the adversarial examples to stay within epsilon-ball and valid pixel range\n",
        "        adv_x = x + clip_epsilon(adv_x-x, epsilon)\n",
        "        adv_x = tf.clip_by_value(adv_x, clip_min, clip_max)\n",
        "\n",
        "    return adv_x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_pgd(model, dataset, epsilon=8, alpha=2, attack_iters=50, restarts=1, batch_size=64):\n",
        "    adversarial_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "    total_batches = sum(1 for _ in dataset.batch(batch_size))\n",
        "    print(f\"\\nEvaluating PGD-{attack_iters}-{restarts} on {total_batches} batches...\")\n",
        "\n",
        "    progbar = tf.keras.utils.Progbar(total_batches * restarts)\n",
        "    batch_count = 0\n",
        "\n",
        "    # Evaluate clean accuracy using model.evaluate\n",
        "    clean_results = model.evaluate(dataset.batch(batch_size), verbose=0)\n",
        "    standard_accuracy = clean_results[1]  # Assuming the accuracy is the second metric\n",
        "\n",
        "    for batch_num, (x, y) in enumerate(dataset.batch(batch_size)):\n",
        "        best_adv_x = x\n",
        "        for restart_num in range(restarts):\n",
        "            adv_x = pgd_attack(model, x, y, epsilon, alpha, attack_iters)\n",
        "\n",
        "            # Keep adversarial examples where model predictions are incorrect\n",
        "            incorrect_preds = tf.argmax(model(adv_x, training=False), axis=1) != tf.argmax(y, axis=1)\n",
        "            incorrect_preds = tf.reshape(incorrect_preds, [-1, 1, 1, 1])\n",
        "            best_adv_x = tf.where(incorrect_preds, adv_x, best_adv_x)\n",
        "\n",
        "            progbar.update(batch_count + 1)\n",
        "            batch_count += 1\n",
        "\n",
        "        logits_adv = model(best_adv_x, training=False)\n",
        "        adversarial_acc_metric.update_state(y, logits_adv)\n",
        "\n",
        "    adversarial_accuracy = adversarial_acc_metric.result().numpy()\n",
        "\n",
        "    print(f\"PGD-{attack_iters}-{restarts} Evaluation complete. \\nValidation Accuracy: {100.0*standard_accuracy:.2f}%, PGD-{attack_iters}-{restarts} Adversarial Accuracy: {100.0*adversarial_accuracy:.2f}%\")\n",
        "    return {\"standard_accuracy\": standard_accuracy, \"adversarial_accuracy\": adversarial_accuracy}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tLnIKFK8APt6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Checkpoint\n",
        "\n",
        "\n",
        "class AdversarialCheckpoint(Callback):\n",
        "    def __init__(self, dataset, epsilon=8.0, alpha=2.0, attack_iters=50, restarts=1, adv_batch_size=64):\n",
        "        \"\"\"\n",
        "        Callback to evaluate model performance on adversarial examples after training.\n",
        "\n",
        "        :param dataset: tf.data.Dataset for evaluation.\n",
        "        :param epsilon: Maximum perturbation for PGD attack.\n",
        "        :param alpha: Step size for PGD attack.\n",
        "        :param attack_iters: Number of iterations for PGD attack.\n",
        "        :param restarts: Number of restarts for PGD attack.\n",
        "        :param batch_size: Batch size for evaluation.\n",
        "        \"\"\"\n",
        "        super(AdversarialCheckpoint, self).__init__()\n",
        "        self.dataset = dataset\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.attack_iters = attack_iters\n",
        "        self.restarts = restarts\n",
        "        self.adv_batch_size = adv_batch_size\n",
        "        self.adv_accuracy = 0.0\n",
        "        self.test_accuracy = 0.0\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        \"\"\"\n",
        "        Called at the end of training. Evaluates the model on both clean and adversarial examples.\n",
        "        \"\"\"\n",
        "        # Evaluate the model on adversarial examples using PGD with multiple restarts\n",
        "        accuracies = evaluate_pgd(\n",
        "            model=self.model, dataset=self.dataset, epsilon=self.epsilon, alpha=self.alpha,\n",
        "            attack_iters=self.attack_iters, restarts=self.restarts, adv_batch_size=self.adv_batch_size\n",
        "        )\n",
        "\n",
        "        self.adv_accuracy = accuracies[\"adversarial_accuracy\"]\n",
        "        self.test_accuracy = accuracies[\"standard_accuracy\"]\n",
        "\n",
        "\n",
        "\n",
        "class AdversarialCheckpoint_Epochs(Callback):\n",
        "    def __init__(self, dataset, epsilon=8.0, alpha=2.0, attack_iters=50, restarts=1, adv_batch_size=64):\n",
        "        \"\"\"\n",
        "        Callback to evaluate model performance on adversarial examples after each epoch.\n",
        "\n",
        "        :param dataset: tf.data.Dataset for evaluation.\n",
        "        :param epsilon: Maximum perturbation for PGD attack.\n",
        "        :param alpha: Step size for PGD attack.\n",
        "        :param attack_iters: Number of iterations for PGD attack.\n",
        "        :param restarts: Number of restarts for PGD attack.\n",
        "        :param batch_size: Batch size for evaluation.\n",
        "        \"\"\"\n",
        "        super(AdversarialCheckpoint_Epochs, self).__init__()\n",
        "        self.dataset = dataset\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.attack_iters = attack_iters\n",
        "        self.restarts = restarts\n",
        "        self.adv_batch_size = adv_batch_size\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \"\"\"\n",
        "        Called at the end of each epoch. Evaluates the model on both clean and adversarial examples.\n",
        "        \"\"\"\n",
        "        # Evaluate the model on adversarial examples using PGD with multiple restarts\n",
        "        accuracies = evaluate_pgd(\n",
        "            model=self.model, dataset=self.dataset, epsilon=self.epsilon, alpha=self.alpha,\n",
        "            attack_iters=self.attack_iters, restarts=self.restarts, batch_size=self.adv_batch_size\n",
        "        )\n",
        "\n",
        "        self.adv_accuracy = accuracies[\"adversarial_accuracy\"]\n",
        "        self.test_accuracy = accuracies[\"standard_accuracy\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AL0nlzrHVUTb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test to use the architectures\n",
        "\n",
        "if False:\n",
        "    model = create_wide_residual_network(input_dim=(32, 32, 3), nb_classes=10, N=4, k=8, dropout=0.0, activation=activation)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "    adv_checkpoint = AdversarialCheckpoint_Epochs(dataset=dataset_test, epsilon=8.0, alpha=2.0, attack_iters=50, restarts=1, adv_batch_size=1024)\n",
        "    model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test), callbacks=[adv_checkpoint])\n",
        "\n"
      ],
      "metadata": {
        "id": "cs_RjCQDLtYk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cyclic learning rate\n",
        "\n",
        "class CyclicLR(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\"\n",
        "    Cyclic learning rate scheduler.\n",
        "\n",
        "    This scheduler varies the learning rate between a maximum and a minimum value in a cosine\n",
        "    wave pattern. The learning rate starts at `lr_max` and gradually decreases to `lr_min`,\n",
        "    then goes back to `lr_max` in a cyclic fashion.\n",
        "\n",
        "    Args:\n",
        "        lr_max (float): The maximum learning rate.\n",
        "        lr_min (float): The minimum learning rate.\n",
        "        nb_epochs (int): The number of epochs over which the learning rate cycles.\n",
        "    \"\"\"\n",
        "    def __init__(self, lr_max, lr_min, nb_epochs):\n",
        "        self.lr_max = lr_max\n",
        "        self.lr_min = lr_min\n",
        "        self.nb_epochs = nb_epochs\n",
        "\n",
        "    def __call__(self, step):\n",
        "        \"\"\"\n",
        "        Calculate the learning rate for a given step.\n",
        "        Args:\n",
        "            step (int): The current training step.\n",
        "\n",
        "        Returns:\n",
        "            float: The calculated learning rate.\n",
        "        \"\"\"\n",
        "        # Calculate the current epoch based on the step\n",
        "        epoch = tf.cast(step // self.nb_epochs, dtype=tf.float32)\n",
        "\n",
        "        # Calculate the position in the current cycle\n",
        "        cycle = tf.constant(np.pi, dtype=tf.float32) * epoch / tf.cast(self.nb_epochs, dtype=tf.float32)\n",
        "\n",
        "        # Calculate and return the cyclic learning rate\n",
        "        return self.lr_min + 0.5 * (self.lr_max - self.lr_min) * (1 + tf.math.cos(cycle))\n",
        "\n",
        "# Parameters for the cyclic learning rate\n",
        "lr_max = 0.2\n",
        "lr_min = 0.001\n",
        "nb_epochs = epochs\n",
        "\n",
        "# Create an instance of the CyclicLR scheduler\n",
        "cyclic_lr_schedule = CyclicLR(lr_max, lr_min, nb_epochs)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ao-cGFAxTIwM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title $l^p$-FGSM\n",
        "\n",
        "\n",
        "class Lp_FGSM(Model):\n",
        "    \"\"\"\n",
        "    Lp Fast Gradient Sign Method (FGSM) adversarial training model.\n",
        "\n",
        "    This class extends the Keras Model class, enabling the creation of adversarial examples\n",
        "    based on an Lp norm during training.\n",
        "\n",
        "    Attributes:\n",
        "        base_model: The underlying model for making predictions.\n",
        "        p (float): The norm degree for Lp norm.\n",
        "        eps (float): Maximum perturbation allowed for adversarial examples.\n",
        "        cce (tf.keras.losses.Loss): Custom categorical cross-entropy loss function.\n",
        "        add_noise (bool): Flag to add noise to the inputs for adversarial generation.\n",
        "        vareps (float): A small constant for numerical stability in norm calculations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_model, p=64.0, eps=8.0, cce=None, add_noise=True, vareps=1e-12, *args, **kwargs):\n",
        "        super(Lp_FGSM, self).__init__(*args, **kwargs)\n",
        "        self.base_model = base_model  # The underlying model for predictions\n",
        "        self.p = tf.constant(p, dtype=tf.float32)\n",
        "        self.q = self.p / (self.p - 1.0)\n",
        "        self.eps = tf.constant(eps / 255.0, dtype=tf.float32)\n",
        "        self.vareps = tf.constant(vareps, dtype=tf.float32)\n",
        "        self.add_noise = add_noise\n",
        "        self.cce = cce if cce is not None else tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def call(self, inputs, training=True):\n",
        "        return self.base_model(inputs, training=training)\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        Custom training step for the Lp_FGSM adversarial training.\n",
        "\n",
        "        Args:\n",
        "            data: Tuple of (input data, labels).\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping metric names to their current value.\n",
        "        \"\"\"\n",
        "        x, y = data  # Unpack the data\n",
        "        probs = self.base_model(x, training=True)\n",
        "        # Generate adversarial examples\n",
        "        if self.add_noise:\n",
        "            # Add random noise and compute the Lp norm perturbation\n",
        "            x_rnd = tf.random.uniform(tf.shape(x), minval=-1.0, maxval=1.0)\n",
        "            x_adv = x + self.eps * x_rnd\n",
        "            Ups_ = tf.pow(tf.reduce_sum(tf.pow(tf.abs(x_rnd), self.q), axis=[1, 2, 3], keepdims=True), 1.0 / self.q)\n",
        "            Upsilon = tf.pow(self.vareps + tf.abs(x_rnd) / (Ups_), self.q - 1.0)\n",
        "            x_rnd = tf.sign(x_rnd) * Upsilon\n",
        "            x_aug = x + self.eps * x_rnd\n",
        "        else:\n",
        "            x_aug = x\n",
        "            x_adv = x\n",
        "\n",
        "        # Gradient computation for the augmented data\n",
        "        with tf.GradientTape(watch_accessed_variables=False, persistent=False) as inner_tape:\n",
        "            inner_tape.watch(x_aug)\n",
        "            probs_aug = self.base_model(x_aug, training=True)\n",
        "            loss_sum = self.cce(y, probs_aug)\n",
        "        dlx = inner_tape.gradient(loss_sum, x_aug)\n",
        "        dlxq = tf.pow(tf.reduce_sum(tf.pow(tf.abs(dlx), self.q), axis=[1, 2, 3], keepdims=True), 1.0 / self.q)\n",
        "        Upsilon = tf.pow(self.vareps + tf.abs(dlx) / (dlxq), self.q - 1.0)\n",
        "        dlxn = tf.sign(dlx) * Upsilon\n",
        "        x_adv += self.eps * dlxn\n",
        "\n",
        "        # Training step for the adversarial data\n",
        "        with tf.GradientTape(watch_accessed_variables=False, persistent=False) as tape:\n",
        "            tape.watch(self.base_model.trainable_variables)\n",
        "            probs_adv = self.base_model(x_adv, training=True)\n",
        "            losses_adv = self.cce(y, probs_adv)\n",
        "            loss_total = tf.reduce_mean(losses_adv)\n",
        "\n",
        "        grads = tape.gradient(loss_total, self.base_model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.base_model.trainable_variables))\n",
        "        self.compiled_metrics.update_state(y, probs)\n",
        "        return {m.name: m.result() for m in self.metrics}\n"
      ],
      "metadata": {
        "id": "_ol6r-2bWA_R",
        "cellView": "form"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training module\n",
        "def train_model(dataset_name, datagen, x_test, y_test, eps=8.0, vareps=1e-12, p=32.0, add_noise=True, epochs=30, learning_rate=0.001, weight_decay=5e-4, batch_size=64, dropout=0.0, pretrain_epochs=0, cyclic_lr=False):\n",
        "    \"\"\"\n",
        "    Trains a neural network model based on the specified dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset_name: String name of the dataset (e.g., 'SVHN', 'CIFAR10', 'CIFAR100').\n",
        "        datagen: Data generator for training (e.g., with data augmentation).\n",
        "        x_test, y_test: Test dataset.\n",
        "        eps: Perturbation limit for adversarial training.\n",
        "        vareps: A small constant for numerical stability in adversarial training.\n",
        "        p: The norm degree for Lp norm in adversarial training.\n",
        "        add_noise: Flag to add noise during adversarial training.\n",
        "        epochs: Number of training epochs.\n",
        "        learning_rate: Learning rate for the optimizer.\n",
        "        weight_decay: Weight decay for L2 regularization.\n",
        "        batch_size: Batch size for training.\n",
        "        dropout: Dropout rate for the model.\n",
        "        pretrain: If True, pretrains the model.\n",
        "        cyclic_lr: If True, uses cyclic learning rate; otherwise uses a constant learning rate.\n",
        "\n",
        "    Returns:\n",
        "        adv_accuracy_linf: Adversarial accuracy on Linf norm perturbed data.\n",
        "        test_accuracy: Accuracy on the clean test dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Select the appropriate model based on the dataset\n",
        "    if dataset_name == 'SVHN':\n",
        "        # PreActResNet18 for SVHN dataset\n",
        "        num_classes = 10\n",
        "        init_shape = (32, 32, 3)\n",
        "        model = PreActResNet18(input_shape=init_shape, num_classes=num_classes, weight_decay=weight_decay, dropout_rate=dropout, activation='relu')\n",
        "    elif dataset_name in ['CIFAR10', 'CIFAR100']:\n",
        "        # WideResNet for CIFAR datasets\n",
        "        num_classes = 10 if dataset_name == 'CIFAR10' else 100\n",
        "        init_shape = (32, 32, 3)\n",
        "        model = create_wide_residual_network(init_shape, nb_classes=num_classes, N=4, k=8, dropout=dropout, weight_decay=weight_decay, activation='relu')\n",
        "    else:\n",
        "        raise ValueError(\"Unknown dataset\")\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate, weight_decay=weight_decay), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Pretrain the model if specified\n",
        "    if pretrain_epochs>0:\n",
        "        model.fit(datagen.flow(x_train, y_train, batch_size=batch_size), validation_data=(x_test, y_test), epochs=pretrain_epochs)\n",
        "\n",
        "    # Initialize and compile the Lp_FGSM model for adversarial training\n",
        "    model_lp = Lp_FGSM(base_model=model,p=p, eps=eps, vareps=vareps, add_noise=add_noise)\n",
        "    if cyclic_lr:\n",
        "        # Use a cyclic learning rate schedule if specified\n",
        "        model_lp.compile(optimizer=SGD(learning_rate=cyclic_lr_schedule, weight_decay=weight_decay, momentum=0.9), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    else:\n",
        "        # Use a constant learning rate otherwise\n",
        "        model_lp.compile(optimizer=Adam(learning_rate=learning_rate, weight_decay=weight_decay), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Set up adversarial checkpoint callback\n",
        "    dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "    adv_callback = AdversarialCheckpoint_Epochs(dataset=dataset_test, epsilon=eps, attack_iters=50, restarts=1, adv_batch_size=1024)\n",
        "    callbacks_list = [adv_callback]\n",
        "\n",
        "    # Train the model using adversarial training\n",
        "    model_lp.fit(datagen.flow(x_train, y_train, batch_size=batch_size), epochs=epochs, validation_data=(x_test, y_test), callbacks=callbacks_list, verbose=1)\n",
        "\n",
        "    # Extract and return the final adversarial and clean test accuracies\n",
        "    adv_accuracy_linf = adv_callback.adv_accuracy\n",
        "    test_accuracy = adv_callback.test_accuracy\n",
        "\n",
        "    return adv_accuracy_linf, test_accuracy\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "I11VqEq-Ydv5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adv_accuracy_linf, test_accuracy = train_model(dataset_name, datagen, x_test, y_test, eps=8.0, vareps=1e-12, p=64.0, add_noise=True, epochs=epochs, learning_rate=0.001, weight_decay=weight_decay, batch_size=batch_size, dropout=dropout, pretrain_epochs=00, cyclic_lr=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIbqCEUFRnMs",
        "outputId": "ed5d9b9f-9564-4742-cf4b-b4ab01174753"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wide Residual Network-28-8 created.\n",
            "Epoch 1/30\n",
            "  6/391 [..............................] - ETA: 35s - accuracy: 0.1823"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 0.0713s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - ETA: 0s - accuracy: 0.3276\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 249s 20s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 42.09%, PGD-50-1 Adversarial Accuracy: 29.13%\n",
            "391/391 [==============================] - 344s 802ms/step - accuracy: 0.3276 - val_loss: 12.7531 - val_accuracy: 0.4209\n",
            "Epoch 2/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.4270\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 174s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 48.20%, PGD-50-1 Adversarial Accuracy: 31.67%\n",
            "391/391 [==============================] - 212s 542ms/step - accuracy: 0.4270 - val_loss: 15.7474 - val_accuracy: 0.4819\n",
            "Epoch 3/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.4806\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 51.84%, PGD-50-1 Adversarial Accuracy: 34.24%\n",
            "391/391 [==============================] - 212s 542ms/step - accuracy: 0.4806 - val_loss: 18.7284 - val_accuracy: 0.5184\n",
            "Epoch 4/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.5251\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 59.49%, PGD-50-1 Adversarial Accuracy: 36.83%\n",
            "391/391 [==============================] - 212s 542ms/step - accuracy: 0.5251 - val_loss: 21.7210 - val_accuracy: 0.5949\n",
            "Epoch 5/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.5590\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 62.24%, PGD-50-1 Adversarial Accuracy: 38.16%\n",
            "391/391 [==============================] - 211s 542ms/step - accuracy: 0.5590 - val_loss: 24.7524 - val_accuracy: 0.6224\n",
            "Epoch 6/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.5917\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 60.44%, PGD-50-1 Adversarial Accuracy: 38.99%\n",
            "391/391 [==============================] - 212s 542ms/step - accuracy: 0.5917 - val_loss: 27.8126 - val_accuracy: 0.6044\n",
            "Epoch 7/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.6047\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 64.26%, PGD-50-1 Adversarial Accuracy: 39.95%\n",
            "391/391 [==============================] - 211s 542ms/step - accuracy: 0.6047 - val_loss: 30.8749 - val_accuracy: 0.6426\n",
            "Epoch 8/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.6298\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 67.27%, PGD-50-1 Adversarial Accuracy: 41.07%\n",
            "391/391 [==============================] - 211s 542ms/step - accuracy: 0.6298 - val_loss: 33.9153 - val_accuracy: 0.6727\n",
            "Epoch 9/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.6428\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 69.21%, PGD-50-1 Adversarial Accuracy: 41.66%\n",
            "391/391 [==============================] - 212s 542ms/step - accuracy: 0.6428 - val_loss: 36.9811 - val_accuracy: 0.6921\n",
            "Epoch 10/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.6598\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 69.79%, PGD-50-1 Adversarial Accuracy: 42.39%\n",
            "391/391 [==============================] - 212s 542ms/step - accuracy: 0.6598 - val_loss: 39.9976 - val_accuracy: 0.6979\n",
            "Epoch 11/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.6741\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 69.33%, PGD-50-1 Adversarial Accuracy: 41.64%\n",
            "391/391 [==============================] - 212s 542ms/step - accuracy: 0.6741 - val_loss: 43.0010 - val_accuracy: 0.6934\n",
            "Epoch 12/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.6839\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 71.35%, PGD-50-1 Adversarial Accuracy: 42.35%\n",
            "391/391 [==============================] - 212s 542ms/step - accuracy: 0.6839 - val_loss: 45.9861 - val_accuracy: 0.7135\n",
            "Epoch 13/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.6917\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 71.44%, PGD-50-1 Adversarial Accuracy: 43.74%\n",
            "391/391 [==============================] - 212s 542ms/step - accuracy: 0.6917 - val_loss: 49.0240 - val_accuracy: 0.7144\n",
            "Epoch 14/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.6999\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 72.58%, PGD-50-1 Adversarial Accuracy: 44.06%\n",
            "391/391 [==============================] - 212s 542ms/step - accuracy: 0.6999 - val_loss: 52.0392 - val_accuracy: 0.7258\n",
            "Epoch 15/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.7130\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 73.85%, PGD-50-1 Adversarial Accuracy: 44.16%\n",
            "391/391 [==============================] - 212s 542ms/step - accuracy: 0.7130 - val_loss: 55.0312 - val_accuracy: 0.7385\n",
            "Epoch 16/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.7200\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 174s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 74.10%, PGD-50-1 Adversarial Accuracy: 44.14%\n",
            "391/391 [==============================] - 212s 543ms/step - accuracy: 0.7200 - val_loss: 58.0229 - val_accuracy: 0.7410\n",
            "Epoch 17/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.7269\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 73.85%, PGD-50-1 Adversarial Accuracy: 45.00%\n",
            "391/391 [==============================] - 212s 543ms/step - accuracy: 0.7269 - val_loss: 61.1267 - val_accuracy: 0.7384\n",
            "Epoch 18/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.7341\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 174s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 74.80%, PGD-50-1 Adversarial Accuracy: 44.67%\n",
            "391/391 [==============================] - 213s 545ms/step - accuracy: 0.7341 - val_loss: 64.2069 - val_accuracy: 0.7480\n",
            "Epoch 19/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.7410\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 174s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 76.04%, PGD-50-1 Adversarial Accuracy: 44.98%\n",
            "391/391 [==============================] - 212s 543ms/step - accuracy: 0.7410 - val_loss: 67.2831 - val_accuracy: 0.7604\n",
            "Epoch 20/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.7495\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 174s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 76.27%, PGD-50-1 Adversarial Accuracy: 44.12%\n",
            "391/391 [==============================] - 212s 544ms/step - accuracy: 0.7495 - val_loss: 70.3076 - val_accuracy: 0.7629\n",
            "Epoch 21/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.7545\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 174s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 76.84%, PGD-50-1 Adversarial Accuracy: 45.00%\n",
            "391/391 [==============================] - 212s 544ms/step - accuracy: 0.7545 - val_loss: 73.4228 - val_accuracy: 0.7684\n",
            "Epoch 22/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.7602\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 174s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 76.06%, PGD-50-1 Adversarial Accuracy: 45.70%\n",
            "391/391 [==============================] - 212s 544ms/step - accuracy: 0.7602 - val_loss: 76.5570 - val_accuracy: 0.7606\n",
            "Epoch 23/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.7665\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 174s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 76.02%, PGD-50-1 Adversarial Accuracy: 45.52%\n",
            "391/391 [==============================] - 212s 544ms/step - accuracy: 0.7665 - val_loss: 79.5958 - val_accuracy: 0.7602\n",
            "Epoch 24/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.7731\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 174s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 77.84%, PGD-50-1 Adversarial Accuracy: 46.27%\n",
            "391/391 [==============================] - 212s 544ms/step - accuracy: 0.7731 - val_loss: 82.7088 - val_accuracy: 0.7784\n",
            "Epoch 25/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.7769\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 174s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 77.75%, PGD-50-1 Adversarial Accuracy: 46.06%\n",
            "391/391 [==============================] - 212s 544ms/step - accuracy: 0.7769 - val_loss: 85.7798 - val_accuracy: 0.7776\n",
            "Epoch 26/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.7822\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 77.02%, PGD-50-1 Adversarial Accuracy: 46.44%\n",
            "391/391 [==============================] - 212s 543ms/step - accuracy: 0.7822 - val_loss: 88.8767 - val_accuracy: 0.7702\n",
            "Epoch 27/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.7894\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 78.07%, PGD-50-1 Adversarial Accuracy: 45.40%\n",
            "391/391 [==============================] - 212s 542ms/step - accuracy: 0.7894 - val_loss: 91.9188 - val_accuracy: 0.7807\n",
            "Epoch 28/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.7951\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 77.81%, PGD-50-1 Adversarial Accuracy: 45.99%\n",
            "391/391 [==============================] - 211s 542ms/step - accuracy: 0.7951 - val_loss: 95.0324 - val_accuracy: 0.7781\n",
            "Epoch 29/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.8005\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 77.34%, PGD-50-1 Adversarial Accuracy: 45.65%\n",
            "391/391 [==============================] - 211s 542ms/step - accuracy: 0.8005 - val_loss: 98.1222 - val_accuracy: 0.7734\n",
            "Epoch 30/30\n",
            "391/391 [==============================] - ETA: 0s - accuracy: 0.8055\n",
            "Evaluating PGD-50-1 on 10 batches...\n",
            "10/10 [==============================] - 173s 17s/step\n",
            "PGD-50-1 Evaluation complete. \n",
            "Validation Accuracy: 78.65%, PGD-50-1 Adversarial Accuracy: 46.01%\n",
            "391/391 [==============================] - 212s 542ms/step - accuracy: 0.8055 - val_loss: 101.2029 - val_accuracy: 0.7865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Figure 6 subplot\n",
        "\n",
        "\n",
        "# Initialize matrices to store results\n",
        "num_seeds = 5\n",
        "epochs= 30\n",
        "vareps=0.0\n",
        "dropout=0.0\n",
        "weight_decay=0.0\n",
        "eps=8.0\n",
        "add_noise=False\n",
        "batch_size=64\n",
        "\n",
        "cyclic_lr=True\n",
        "ps = [2, 4, 8, 16, 32, 64, 128, 256]\n",
        "A_adv = np.zeros((len(ps), num_seeds))\n",
        "A_test = np.zeros((len(ps), num_seeds))\n",
        "\n",
        "for seed in range(num_seeds):\n",
        "    # Set the random seed for reproducibility\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    # Loop over each value of p\n",
        "    for i, p in enumerate(ps):\n",
        "    # Loop over each seed value\n",
        "        # Run the training model, assuming that other parameters are set or passed explicitly\n",
        "        adv_accuracy_linf, test_accuracy = train_model(dataset_name, datagen, x_test, y_test,eps=eps, vareps=vareps, p=p, add_noise=add_noise, epochs=epochs, weight_decay=weight_decay, batch_size=batch_size, dropout=dropout, pretrain_epochs=0)\n",
        "\n",
        "        # Store the results in the matrices\n",
        "        A_adv[i, seed] = adv_accuracy_linf\n",
        "        A_test[i, seed] = test_accuracy\n",
        "\n",
        "    # Save matrices to disk\n",
        "    np.save('CIFAR10_Fig6_adv.npy', A_adv)\n",
        "    np.save('CIFAR10_Fig6_test.npy', A_test)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K1QKEZu3be_1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}